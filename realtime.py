"""
MLX Whisper å³æ™‚èªéŸ³è¾¨è­˜ï¼ˆä½¿ç”¨ Apple Silicon GPUï¼‰
æ”¯æ´ä½¿ç”¨æœ¬åœ°è½‰æ›çš„æ¨¡å‹

ä½¿ç”¨æ–¹å¼:
  # ä½¿ç”¨é è¨­è¨­å®šï¼ˆtranscribe, è‡ªå‹•åµæ¸¬èªè¨€ï¼‰
  uv run python realtime.py
  
  # æŒ‡å®šæ¨¡å‹
  uv run python realtime.py --model whisper-large-v2-taiwanese-hakka-v1-mlx
  
  # ç¿»è­¯æˆè‹±æ–‡
  uv run python realtime.py --task translate
  
  # æŒ‡å®šèªè¨€
  uv run python realtime.py --language zh
  
  # åˆ—å‡ºå¯ç”¨æ¨¡å‹
  uv run python realtime.py --list
"""
import argparse
import sys
import numpy as np
import pyaudio
import mlx_whisper
from pathlib import Path

# ===========================================
# è·¯å¾‘è¨­å®š
# ===========================================
SCRIPT_DIR = Path(__file__).parent
MODELS_DIR = SCRIPT_DIR / "models"

# ===========================================
# éŒ„éŸ³è¨­å®š
# ===========================================
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 16000
CHUNK = 1024
SILENCE_THRESHOLD = 500
SILENCE_DURATION = 1.5


def list_available_models() -> list[str]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æœ¬åœ°æ¨¡å‹"""
    if not MODELS_DIR.exists():
        return []
    
    models = []
    for path in MODELS_DIR.iterdir():
        if path.is_dir():
            config_file = path / "config.json"
            weights_file = path / "weights.npz"
            if config_file.exists() and weights_file.exists():
                models.append(path.name)
    
    return sorted(models)


def get_model_path(model_name: str) -> Path:
    """å–å¾—æ¨¡å‹è·¯å¾‘"""
    # å¦‚æœæ˜¯å®Œæ•´è·¯å¾‘
    if "/" in model_name or model_name.startswith("."):
        return Path(model_name)
    
    # å¦å‰‡åœ¨ models ç›®éŒ„ä¸­å°‹æ‰¾
    model_path = MODELS_DIR / model_name
    
    # å˜—è©¦åŠ ä¸Š -mlx å¾Œç¶´
    if not model_path.exists() and not model_name.endswith("-mlx"):
        model_path = MODELS_DIR / f"{model_name}-mlx"
    
    return model_path


def get_audio_level(data):
    """è¨ˆç®—éŸ³é‡"""
    samples = np.frombuffer(data, dtype=np.int16)
    return np.abs(samples).mean()


def record_until_silence(stream):
    """éŒ„éŸ³ç›´åˆ°éœéŸ³"""
    frames = []
    silent_chunks = 0
    chunks_for_silence = int(SILENCE_DURATION * RATE / CHUNK)
    is_speaking = False
    
    while True:
        data = stream.read(CHUNK, exception_on_overflow=False)
        level = get_audio_level(data)
        
        if level > SILENCE_THRESHOLD:
            is_speaking = True
            silent_chunks = 0
            frames.append(data)
        elif is_speaking:
            frames.append(data)
            silent_chunks += 1
            if silent_chunks > chunks_for_silence:
                break
    
    return b''.join(frames)


def transcribe_audio(audio_data, model_path: str, language: str | None, task: str):
    """ä½¿ç”¨ MLX Whisper è¾¨è­˜"""
    audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
    
    kwargs = {
        "path_or_hf_repo": model_path,
        "task": task,
    }
    
    # åªæœ‰åœ¨æŒ‡å®šèªè¨€æ™‚æ‰å‚³å…¥
    if language:
        kwargs["language"] = language
    
    result = mlx_whisper.transcribe(audio_np, **kwargs)
    
    return result["text"].strip()


def main():
    parser = argparse.ArgumentParser(
        description="MLX Whisper å³æ™‚èªéŸ³è¾¨è­˜ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # åŸºæœ¬ä½¿ç”¨ï¼ˆè‡ªå‹•åµæ¸¬èªè¨€ï¼Œç´”è½‰éŒ„ï¼‰
  uv run python realtime.py
  
  # ç¿»è­¯æˆè‹±æ–‡
  uv run python realtime.py --task translate
  
  # æŒ‡å®šèªè¨€ç‚ºä¸­æ–‡
  uv run python realtime.py --language zh
  
  # ä½¿ç”¨ç‰¹å®šæ¨¡å‹
  uv run python realtime.py --model whisper-large-v2-taiwanese-hakka-v1-mlx
  
  # çµ„åˆä½¿ç”¨
  uv run python realtime.py -m whisper-large-v2-taiwanese-hakka-v1-mlx -l zh -t transcribe
"""
    )
    parser.add_argument(
        "--model", "-m",
        type=str,
        default=None,
        help="æ¨¡å‹åç¨±æˆ–è·¯å¾‘ï¼ˆé è¨­ï¼šä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨çš„æœ¬åœ°æ¨¡å‹ï¼‰",
    )
    parser.add_argument(
        "--task", "-t",
        type=str,
        choices=["transcribe", "translate"],
        default="transcribe",
        help="ä»»å‹™é¡å‹ï¼štranscribeï¼ˆè½‰éŒ„ï¼‰æˆ– translateï¼ˆç¿»è­¯æˆè‹±æ–‡ï¼‰ï¼ˆé è¨­ï¼štranscribeï¼‰",
    )
    parser.add_argument(
        "--language", "-l",
        type=str,
        default=None,
        help="èªè¨€ä»£ç¢¼ï¼Œå¦‚ zh, en, jaï¼ˆé è¨­ï¼šè‡ªå‹•åµæ¸¬ï¼‰",
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="åˆ—å‡ºå¯ç”¨çš„æœ¬åœ°æ¨¡å‹",
    )
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ¨¡å‹
    if args.list:
        models = list_available_models()
        if models:
            print("å¯ç”¨çš„æœ¬åœ°æ¨¡å‹:")
            for m in models:
                print(f"  â€¢ {m}")
        else:
            print("å°šæœªæœ‰è½‰æ›çš„æ¨¡å‹")
            print(f"\nè«‹å…ˆä½¿ç”¨ convert/convert.sh è½‰æ›æ¨¡å‹:")
            print(f"  cd convert")
            print(f"  ./convert.sh <hf-repo>")
        return
    
    # å–å¾—æ¨¡å‹
    available_models = list_available_models()
    
    if args.model:
        model_path = get_model_path(args.model)
    elif available_models:
        # ä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨çš„æ¨¡å‹
        model_path = MODELS_DIR / available_models[0]
        print(f"ä½¿ç”¨æ¨¡å‹: {available_models[0]}")
    else:
        print("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ä»»ä½•æœ¬åœ°æ¨¡å‹")
        print(f"\nè«‹å…ˆè½‰æ›æ¨¡å‹:")
        print(f"  cd convert")
        print(f"  ./convert.sh <hf-repo>")
        print(f"\nç¯„ä¾‹:")
        print(f"  ./convert.sh formospeech/whisper-large-v2-taiwanese-hakka-v1")
        sys.exit(1)
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not model_path.exists():
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¨¡å‹ {model_path}")
        if available_models:
            print(f"\nå¯ç”¨çš„æ¨¡å‹:")
            for m in available_models:
                print(f"  â€¢ {m}")
        sys.exit(1)
    
    # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ
    if not (model_path / "config.json").exists() or not (model_path / "weights.npz").exists():
        print(f"éŒ¯èª¤ï¼šæ¨¡å‹ä¸å®Œæ•´ {model_path}")
        print("éœ€è¦ config.json å’Œ weights.npz")
        sys.exit(1)
    
    # é¡¯ç¤ºè¨­å®š
    task_display = "è½‰éŒ„" if args.task == "transcribe" else "ç¿»è­¯æˆè‹±æ–‡"
    lang_display = args.language if args.language else "è‡ªå‹•åµæ¸¬"
    
    print("=" * 50)
    print("MLX Whisper å³æ™‚èªéŸ³è¾¨è­˜")
    print("ä½¿ç”¨ Apple Silicon GPU åŠ é€Ÿ")
    print("=" * 50)
    print(f"æ¨¡å‹: {model_path.name}")
    print(f"ä»»å‹™: {task_display}")
    print(f"èªè¨€: {lang_display}")
    print("=" * 50)
    print("\nèªªè©±å¾Œï¼Œæ–‡å­—æœƒå³æ™‚é¡¯ç¤º")
    print("æŒ‰ Ctrl+C åœæ­¢\n")
    print("-" * 50)
    
    # åˆå§‹åŒ– PyAudio
    audio = pyaudio.PyAudio()
    stream = audio.open(
        format=FORMAT,
        channels=CHANNELS,
        rate=RATE,
        input=True,
        frames_per_buffer=CHUNK
    )
    
    print("æ­£åœ¨è¼‰å…¥æ¨¡å‹...")
    
    # é ç†±æ¨¡å‹
    dummy = np.zeros(RATE, dtype=np.float32)
    warmup_kwargs = {"path_or_hf_repo": str(model_path), "task": args.task}
    if args.language:
        warmup_kwargs["language"] = args.language
    mlx_whisper.transcribe(dummy, **warmup_kwargs)
    print("æ¨¡å‹è¼‰å…¥å®Œæˆï¼é–‹å§‹ç›£è½...\n")
    
    try:
        while True:
            print("ğŸ¤ ç­‰å¾…èªªè©±...", end="\r")
            audio_data = record_until_silence(stream)
            
            if len(audio_data) > CHUNK * 10:
                print("â³ è¾¨è­˜ä¸­...   ", end="\r")
                text = transcribe_audio(audio_data, str(model_path), args.language, args.task)
                if text:
                    print(f"ğŸ“ {text}")
    
    except KeyboardInterrupt:
        print("\n\næ­£åœ¨é—œé–‰...")
    
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()
        print("å·²åœæ­¢")


if __name__ == "__main__":
    main()
